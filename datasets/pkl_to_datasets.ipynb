{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d0f5fd-9bbf-478c-8daa-5b27b243657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf8882a-d1c4-4dfb-bc9f-ac9552107980",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_methods = ['para', 'gen']\n",
    "datasets = ['ag_news','news_topic', 'trec', 'mnli', 'yahoo', 'tweet_eval_sent', 'yelp']\n",
    "seeds = ['0','1','2']\n",
    "icl_strategies = ['synth_dis', 'cos_sim']\n",
    "methods = ['only', 'uniform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66ba4b6-88d2-4f49-afe6-a477af911bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_to_dataset(path_to_file, path_to_res_csv):\n",
    "    file = open(path_to_file,'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    pattern = r'\\d:'\n",
    "    second_pattern = r'\\d.'\n",
    "    \n",
    "    labels = []\n",
    "    final_sents = []\n",
    "    for response in object_file:\n",
    "        loc = response[0]['generated_text'].rfind('[/INST]') + len('[/INST]') + 1\n",
    "        gen_text = response[0]['generated_text'][loc:]\n",
    "        sents = gen_text.split('\\n')\n",
    "        sents = [x for x in sents if x != \"\"]\n",
    "        tmp_sents = [x.lower() for x in sents]\n",
    "        tmp_sents = [re.sub(pattern, ' ', x) for x in tmp_sents]\n",
    "        tmp_sents = [re.sub(second_pattern, ' ', x) for x in tmp_sents]\n",
    "        tmp_sents = [x for x in tmp_sents if x != \"\"]\n",
    "        tmp_sents = [x for x in tmp_sents if not '=>' in x] # label leakage\n",
    "        tmp_sents = [x for x in tmp_sents if len(x) >= 15] # also potential label leakage\n",
    "        final_sents.extend(tmp_sents)\n",
    "        labels.extend([response[1]]*len(tmp_sents))\n",
    "        \n",
    "    filt_sents = []\n",
    "    for sent in final_sents:\n",
    "        if sent[0] in string.punctuation:\n",
    "            filt_sents.append(sent[1:].rstrip().lstrip())\n",
    "        else:\n",
    "            filt_sents.append(sent.rstrip().lstrip())\n",
    "            \n",
    "    dct_data = {'text': filt_sents, 'label': labels}\n",
    "    df = pd.DataFrame.from_dict(dct_data)\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    \n",
    "    df.sample(frac=1).reset_index(drop=True).to_csv(path_to_res_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61375ba8-11bf-46d1-bad5-3b992728feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_to_dataset_mnli(path_to_file, path_to_res_csv):\n",
    "    file = open(path_to_file,'rb')\n",
    "    object_file = pickle.load(file)\n",
    "    file.close()\n",
    "\n",
    "    pattern = r'\\d:'\n",
    "    second_pattern = r'\\d.'\n",
    "    \n",
    "    labels = []\n",
    "    premises = []\n",
    "    final_sents = []\n",
    "    for response in object_file:\n",
    "        #print(response)\n",
    "        loc = response[0]['generated_text'].rfind('[/INST]') + len('[/INST]') + 1\n",
    "        gen_text = response[0]['generated_text'][loc:]\n",
    "        sents = gen_text.split('\\n')\n",
    "        sents = [x for x in sents if x != \"\"]\n",
    "        tmp_sents = [x.lower() for x in sents]\n",
    "        tmp_sents = [x.replace(\"hypothesis\", \"\") for x in tmp_sents]\n",
    "        tmp_sents = [re.sub(pattern, ' ', x) for x in tmp_sents]\n",
    "        tmp_sents = [re.sub(second_pattern, ' ', x) for x in tmp_sents]\n",
    "\n",
    "        #tmp_sents = [sent[3:] for sent in tmp_sents]\n",
    "        tmp_sents = [x for x in tmp_sents if x != \"\"]\n",
    "        tmp_sents = [x for x in tmp_sents if not 'premise' in x]\n",
    "        tmp_sents = [x for x in tmp_sents if not '=>' in x] # label leakage\n",
    "        tmp_sents = [x for x in tmp_sents if len(x) >= 15] # also potential label leakage\n",
    "        final_sents.extend(tmp_sents)\n",
    "        labels.extend([response[1]['label']]*len(tmp_sents))\n",
    "        premises.extend([response[1]['premise']]*len(tmp_sents))\n",
    "        \n",
    "    filt_sents = []\n",
    "    for sent in final_sents:\n",
    "        if sent[0] in string.punctuation:\n",
    "            filt_sents.append(sent[1:].rstrip().lstrip())\n",
    "        else:\n",
    "            filt_sents.append(sent.rstrip().lstrip())\n",
    "            \n",
    "    dct_data = {'premise': premises, 'label': labels, 'hypothesis': filt_sents}\n",
    "    df = pd.DataFrame.from_dict(dct_data)\n",
    "    df['hypothesis'] = df['hypothesis'].str.lower()\n",
    "    \n",
    "    df.sample(frac=1).reset_index(drop=True).to_csv(path_to_res_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d55737-5476-4abd-89ea-ca32c8f0b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_methods = ['para', 'gen']\n",
    "#datasets = ['ag_news', 'clinc150', 'news_topic', 'trec', 'snips', 'yahoo', 'tweet_eval_sent', 'yelp']\n",
    "datasets = ['tweet_eval_sent']\n",
    "seeds = ['0','1','2']\n",
    "icl_strategies = ['random', 'baseline', 'synth_dis']\n",
    "methods = ['only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66832189-c8b3-4624-8f4b-dd286d2390d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_method in gen_methods:\n",
    "    for dataset in datasets:\n",
    "        for seed in seeds:\n",
    "            for icl_strategy in icl_strategies:\n",
    "                for method in methods:\n",
    "                    path_to_file = os.path.join(dataset, 'collected_data_100', seed, icl_strategy, method + '_' +gen_method+'.pkl')\n",
    "                    path_to_res_csv = os.path.join(dataset, 'collected_data_100', seed, icl_strategy, method + '_' +gen_method+'.csv')\n",
    "                    if dataset == 'mnli':\n",
    "                        pkl_to_dataset_mnli(path_to_file, path_to_res_csv)\n",
    "                    else:\n",
    "                        pkl_to_dataset(path_to_file, path_to_res_csv)\n",
    "                    # print(path_to_file)\n",
    "                    # print(path_to_res_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
